---
title: Capstone Project Integration
---

# Capstone Project Integration: The Complete VLA Pipeline

This section brings everything together. We will now create the master coordination node that executes the plan generated by our Gemini Cognitive Planner. This node will act as the orchestrator, interfacing with the robot's navigation (Nav2), computer vision (Isaac ROS), and manipulation (ROS 2/Gazebo) services to achieve complex, voice-commanded tasks.

## The Master Coordination Node (Behavior Tree or Action Client)

For coordinating complex, sequential tasks with error handling and fallback behaviors, **Behavior Trees** are an excellent choice in ROS 2. Alternatively, a simpler **Action Client** node can be used to sequentially call actions. For this Capstone, we will outline the structure for a master ROS 2 Action Client node that subscribes to the plan from the Gemini planner and executes each step.

Create a Python file, `vla_master_node.py`, in your ROS 2 package.

```python
#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import json
import time

# Import necessary ROS 2 action/service types from previous modules
# For example:
# from nav2_msgs.action import NavigateToPose
# from manipulation_msgs.action import GraspObject # Placeholder for your manipulation action
# from vision_msgs.srv import FindObject # Placeholder for your vision service

class VLAMasterNode(Node):
    def __init__(self):
        super().__init__('vla_master_node')
        self.plan_subscription = self.create_subscription(
            String,
            '/robot_action_sequence',
            self.plan_callback,
            10
        )
        self.get_logger().info('VLA Master Node has been started.')
        self.current_plan = []
        self.plan_executing = False

        # Initialize ROS 2 action clients here
        # self.nav_to_pose_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')
        # self.grasp_object_client = ActionClient(self, GraspObject, 'grasp_object')

    def plan_callback(self, msg):
        if self.plan_executing:
            self.get_logger().warn("New plan received while another plan is executing. Ignoring for now.")
            return

        self.current_plan = json.loads(msg.data)
        self.get_logger().info(f"Received new plan: {self.current_plan}")
        self.plan_executing = True
        self.execute_plan()

    def execute_plan(self):
        self.get_logger().info("Starting plan execution...")
        for action in self.current_plan:
            action_name = action["name"]
            action_args = action.get("args", {})
            self.get_logger().info(f"Executing action: {action_name} with args: {action_args}")

            # Here, you would call the corresponding ROS 2 action client or service
            if action_name == "navigate_to_location":
                location = action_args.get("location")
                self.get_logger().info(f"Navigating to {location}...")
                # self.send_navigation_goal(location) # Call your Nav2 action client
                time.sleep(5) # Simulate navigation time
                self.get_logger().info(f"Finished navigating to {location}.")
            elif action_name == "find_object":
                object_name = action_args.get("object_name")
                self.get_logger().info(f"Finding {object_name} using vision services...")
                # self.call_find_object_service(object_name) # Call your vision service
                time.sleep(3) # Simulate object finding time
                self.get_logger().info(f"Finished finding {object_name}.")
            elif action_name == "grasp_object":
                self.get_logger().info("Grasping object...")
                # self.send_grasp_action() # Call your manipulation action client
                time.sleep(2) # Simulate grasping time
                self.get_logger().info("Finished grasping object.")
            elif action_name == "place_object":
                location = action_args.get("location")
                self.get_logger().info(f"Placing object at {location}...")
                # self.send_place_action(location) # Call your manipulation action client
                time.sleep(3) # Simulate placing time
                self.get_logger().info(f"Finished placing object at {location}.")
            else:
                self.get_logger().warn(f"Unknown action: {action_name}. Skipping.")

        self.get_logger().info("Plan execution complete!")
        self.plan_executing = False

    # Placeholder for actual ROS 2 action/service calls
    # def send_navigation_goal(self, location):
    #     goal_msg = NavigateToPose.Goal()
    #     # ... populate goal_msg based on location
    #     self.nav_to_pose_client.wait_for_server()
    #     self.nav_to_pose_client.send_goal_async(goal_msg)

    # def call_find_object_service(self, object_name):
    #     # ... call vision service
    #     pass

    # def send_grasp_action(self):
    #     # ... send grasp action
    #     pass

    # def send_place_action(self, location):
    #     # ... send place action
    #     pass

def main(args=None):
    rclpy.init(args=args)
    node = VLAMasterNode()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

This master node will subscribe to the `/robot_action_sequence` topic published by our Gemini Cognitive Planner. When a new plan arrives, it parses the sequence of actions and calls the appropriate ROS 2 action clients or services.

## Capstone Project Goals: "Clean the Room"

For our Capstone Project, we aim to enable the humanoid robot to "Clean the Room" based on a voice command. This involves:

-   **Voice Command Reception**: Using Whisper to convert "Clean the room" into text.
-   **Cognitive Planning**: Gemini breaks this down into:
    -   Navigate to specific objects (e.g., "Navigate to table", "Navigate to shelf").
    -   Identify objects (e.g., "Find object: cup", "Find object: block").
    -   Grasp objects.
    -   Place objects (e.g., "Place object: cup at kitchen_sink").
-   **Obstacle Avoidance**: Handled by the Nav2 stack (Module 3).
-   **Object Identification (CV)**: Achieved through Isaac ROS perception services (Module 3).
-   **Manipulation**: Handled by manipulation actions (Module 1, simulated in Gazebo/Unity from Module 2).

This comprehensive integration demonstrates the full power of the VLA pipeline, transforming high-level human commands into autonomous robot behavior.
